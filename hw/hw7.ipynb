{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Umair Hussain\n",
        "###hussa906"
      ],
      "metadata": {
        "id": "wQWxShu2puYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.\n",
        "## Describe how the posterior predictive distribution is created for mixture models\n",
        "1. Randomly select a draw from the posterior distribution of parameters obtained, let's say from a Markov chain.\n",
        "\n",
        "2. Extract the parameters μ, σ, and weights from this draw.\n",
        "\n",
        "3. Define a mixture model using these parameters. Each component of the mixture model is a normal distribution with location μᵢ, standard deviation σᵢ, and weight wᵢ, where i represents the index of the component.\n",
        "\n",
        "4. Plot the mixture model to represent the posterior predictive distribution.\n",
        "\n",
        "5. Repeat steps 1-4 for multiple draws to get several distributions.\n",
        "\n",
        "6. Now average the distributions obtained in step 5 to find the mean of the posterior predictive distribution.\n",
        "\n",
        "So, in summary, the process involves sampling from the posterior distribution, constructing mixture models with the sampled parameters, plotting these models, and then potentially averaging the resulting distributions to obtain the mean of the posterior predictive distribution."
      ],
      "metadata": {
        "id": "jbdOJEQrp54E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.\n",
        "## Describe how the posterior predictive distribution is created in general\n",
        "\n",
        "In general the distribution is creageted by marginalizing $\\theta$ out of the distribution. This can be seen via the following integral:\n",
        "\n",
        "$$p(\\tilde y| x) = \\int p(\\tilde y| \\theta) p(\\theta|x)d\\theta$$\n",
        "\n",
        "However as bayesians in practice we use MCMC methods\n",
        "1. By employing Markov Chain Monte Carlo (MCMC) methodologies, parameters are randomly drawn from the posterior distribution. This iterative procedure generates parameter sets that align with observed data and prior assumptions, encapsulating the uncertainties in parameter estimation.\n",
        "\n",
        "2. Utilizing the sampled parameter sets, new data points are simulated in accordance with the model's specifications. Through this simulation process, synthetic outcomes are produced based on the sampled parameter values, resembling the data-generation mechanism underlying the observed data.\n",
        "\n",
        "3. The simulated data generated from various sets of sampled parameters are consolidated. This amalgamation results in a compilation of simulated datasets, each representing a plausible realization of the model considering the uncertainties in parameter estimation.\n",
        "\n",
        "4. The compiled simulated data is interpreted as an approximation of the posterior predictive distribution. This assortment of simulated datasets characterizes the model's uncertainties and provides insights into the potential range of outcomes. By assessing the variability across these simulated datasets, one gains a comprehensive understanding of the model's predictive behavior and the associated uncertainties in future observations."
      ],
      "metadata": {
        "id": "1AfR3E6crZ2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.\n",
        "## Have a glance through this and then describe how, if you were doing a regression of $y$ on $X$ but $X$ had some missing values, you could perform a Bayesian analysis without throwing away the rows with missing values in $X$\n",
        "\n",
        "\n",
        "Within a Bayesian framework, we handle missing $X$ values by introducing latent variables $v$ to represent the subpopulation with completely missing values. These latent variables are treated as parameters to be inferred through posterior analysis. By assuming a distribution for the missing values and integrating $v$ into the model, we circumvent the need to discard observations with missing data. It's important to note that the same approach can be used for missing values in data that require imputation. However, we should exercise caution regarding the Missing Completely at Random assumption, as deviations from this assumption may affect the validity of our inferences. Through Bayesian methods like PyMC, we estimate model parameters, including coefficients and the missingness mechanism $v$. Subsequently, we can provide posterior Bayesian credibility intervals for the missing values using techniques such as MCMC. This approach facilitates robust regression analysis, ensuring that all available data contributes meaningfully to our understanding."
      ],
      "metadata": {
        "id": "Z7TA_dSG6K2E"
      }
    }
  ]
}
